{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Edit Distance\n",
    "Trying to implement strand isolation using purely Edit Distance. Want to select the same strands together after signatures using some similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aligned_clustering import conduct_align_clustering\n",
    "from utils import get_fastq_records, load_json_file, get_original_strands, get_badread_strand_id, get_recovery_percentage, create_random_strand\n",
    "import Levenshtein\n",
    "import random\n",
    "from Levenshtein import ratio, distance\n",
    "from collections import Counter\n",
    "from seq_stat import align\n",
    "import matplotlib.pyplot as plt\n",
    "import heirarchal_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#records_original = get_fastq_records(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\reads_no_adapters.fastq\")\n",
    "records_original = get_fastq_records(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\reads_adapters.fastq\\reads_adapters.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_strand_ids, coupling_rates, capping_flags, original_strands = get_original_strands(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\original_strands.txt\")\n",
    "\n",
    "strand_ids_synthesized = load_json_file(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\synthesized_uid_reference.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [i for i in records_original if get_badread_strand_id(i) in strand_ids_synthesized]\n",
    "#records = filter_junk_reads(records)\n",
    "seqs = [str(i.seq) for i in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ID reference functions\n",
    "\n",
    "def get_badread_strand_id(record):\n",
    "    return record.description.split()[1].split(',')[0]\n",
    "\n",
    "def get_strand_reference(strand_record, strand_index=True):\n",
    "\n",
    "    strand_id = get_badread_strand_id(strand_record)\n",
    "\n",
    "    if strand_id in strand_ids_synthesized:\n",
    "        if strand_index:\n",
    "            return original_strand_ids.index(strand_ids_synthesized[strand_id])\n",
    "        else: \n",
    "            return original_strands[original_strand_ids.index(strand_ids_synthesized[strand_id])]\n",
    "    print(\"Invalid Strand ID!\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edit_distance_matrix(strands):\n",
    "    \"\"\"\n",
    "    Returns the edit distance matrix for the strands\n",
    "    O(n^2)\n",
    "    \"\"\"\n",
    "\n",
    "    n_strands = len(strands)\n",
    "    edit_distance_matrix = np.zeros([n_strands, n_strands])\n",
    "    for i in range(n_strands - 1):\n",
    "        for j in range(i + 1, n_strands):\n",
    "            edit_distance_matrix[i,j] = edit_distance_matrix[j, i] = ratio(strands[i], strands[j])\n",
    "\n",
    "    return edit_distance_matrix\n",
    "\n",
    "def calculate_centroid(strands):\n",
    "    edit_distance_matrix = get_edit_distance_matrix(strands)\n",
    "\n",
    "    distances = [sum(edit_distance_matrix[i, :]) for i in range(len(edit_distance_matrix))]\n",
    "    return strands[distances.index(min(distances))]\n",
    "\n",
    "\n",
    "def get_mean_edit_distance_cluster(edit_distance_matrix):\n",
    "    distances = [sum(edit_distance_matrix[i, :]) for i in range(len(edit_distance_matrix))]\n",
    "    return np.mean(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total strands 287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [00:00, 35944.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To catch the adapters\n",
    "clusters, centroids = heirarchal_clustering.cluster_trivial(seqs, similarity_threshold=0.75, use_centroids=False, analysis=False)\n",
    "clustered_seqs = [[str(records[i].seq) for i in j] for j in clusters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtien edit distance works pretty well. That's my metric. Kmeans works for now. I'll bring down junk reads etc, remove adapters, and test it again to see how well aggregation works.\n",
    "Maybe try DBscan with Levenshtien distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.9586374695863747 recovered by 0\n",
      "\n",
      "0 0.9408983451536643 recovered by 1\n",
      "\n",
      "3 0.6603325415676959 recovered by 2\n",
      "\n",
      "1 0.6299212598425197 recovered by 3\n",
      "\n",
      "0 0.6745843230403801 recovered by 4\n",
      "\n",
      "4 0.6426858513189448 recovered by 5\n",
      "\n",
      "1 0.9563106796116505 recovered by 6\n",
      "\n",
      "3 0.9582309582309583 recovered by 7\n",
      "\n",
      "2 0.9245283018867925 recovered by 8\n",
      "\n",
      "1 0.6439024390243903 recovered by 9\n",
      "\n",
      "4 0.6616541353383458 recovered by 10\n",
      "\n",
      "4 0.6287425149700598 recovered by 11\n",
      "\n",
      "4 0.5054945054945055 recovered by 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And then merge and validate for top 5\n",
    "\n",
    "original_strand_guessed_best = []\n",
    "\n",
    "for ind, i in enumerate(centroids):\n",
    "    \n",
    "    best_guessed = 0\n",
    "    best_rec = 0.0\n",
    "    for k, original_strand in enumerate(original_strands):\n",
    "        rec = ratio(i, original_strand)\n",
    "\n",
    "        if rec > best_rec:\n",
    "            best_guessed = k\n",
    "            best_rec = rec\n",
    "\n",
    "    original_strand_guessed_best.append(best_guessed)\n",
    "    #if best_rec > 0.9:\n",
    "    #    print(i)\n",
    "    #    print(original_strands[best_guessed])\n",
    "    print(f\"{best_guessed} {best_rec} recovered by {ind}\".format())\n",
    "    #print(len(i))\n",
    "\n",
    "    #print(f\"{len(clusters[ind])} elements in the cluster\")\n",
    "    #print(f\"{np.mean([len(i) for i in clustered_seqs[ind]])} mean length of strand in cluster\")\n",
    "    #print(f\"{np.std([len(i) for i in clustered_seqs[ind]])} mean std of strand in cluster\")\n",
    "    #print(f\"{get_mean_edit_distance_cluster(distance_matrices[ind])} mean edit distance within cluster\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:00, 1182.65it/s]\n"
     ]
    }
   ],
   "source": [
    "clusters_2 = heirarchal_clustering.cluster_trivial(centroids, similarity_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_seqs.sort(key=len, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.5110294117647058 recovered by 0\n",
      "\n",
      "3 1.0 recovered by 1\n",
      "\n",
      "1 1.0 recovered by 2\n",
      "\n",
      "0 0.5243445692883895 recovered by 3\n",
      "\n",
      "4 1.0 recovered by 4\n",
      "\n",
      "2 1.0 recovered by 5\n",
      "\n",
      "0 0.5183823529411765 recovered by 6\n",
      "\n",
      "0 0.4856115107913669 recovered by 7\n",
      "\n",
      "0 1.0 recovered by 8\n",
      "\n",
      "4 0.5054545454545455 recovered by 9\n",
      "\n",
      "1 0.9900497512437811 recovered by 10\n",
      "\n",
      "4 0.46875 recovered by 11\n",
      "\n",
      "1 0.3701657458563536 recovered by 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And then merge and validate for top 5\n",
    "\n",
    "original_strand_guessed_best = []\n",
    "\n",
    "for i in range(len(clustered_seqs)):\n",
    "    if len(clustered_seqs) > 15:\n",
    "        guess = heirarchal_clustering.make_prediction(clustered_seqs[i], sample_size=15)\n",
    "    else:\n",
    "        guess = heirarchal_clustering.make_prediction(clustered_seqs[i], sample_size=len(clustered_seqs[i]))\n",
    "    \n",
    "    best_guessed = 0\n",
    "    best_rec = 0.0\n",
    "    for k, original_strand in enumerate(original_strands):\n",
    "        rec = get_recovery_percentage(guess, original_strand)\n",
    "        rec = align(guess, original_strand)\n",
    "\n",
    "        if rec > best_rec:\n",
    "            best_guessed = k\n",
    "            best_rec = rec\n",
    "\n",
    "    original_strand_guessed_best.append(best_guessed)\n",
    "    print(f\"{best_guessed} {best_rec} recovered by {i}\".format())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs further testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
