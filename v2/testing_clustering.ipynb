{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Edit Distance\n",
    "Trying to implement strand isolation using purely Edit Distance. Want to select the same strands together after signatures using some similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import aligned_clustering\n",
    "from utils import get_fastq_records, load_json_file, get_original_strands, get_badread_strand_id, get_recovery_percentage, create_random_strand, len_histogram, get_sort_by_sublists_length\n",
    "import Levenshtein\n",
    "import random\n",
    "from Levenshtein import ratio, distance\n",
    "from collections import Counter\n",
    "from seq_stat import align\n",
    "import matplotlib.pyplot as plt\n",
    "import heirarchal_clustering\n",
    "import strand_filtering\n",
    "from tqdm.notebook import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "296it [00:00, 21007.36it/s]\n"
     ]
    }
   ],
   "source": [
    "#records_original = get_fastq_records(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\reads_no_adapters.fastq\")\n",
    "records = utils.postprocess_badread_sequencing_data(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\reads_adapters.fastq\\reads_adapters.fastq\", reverse_oriented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_strand_ids, coupling_rates, capping_flags, original_strands = get_original_strands(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\original_strands.txt\")\n",
    "\n",
    "strand_ids_synthesized = load_json_file(r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-30 21.21.00.463318\\synthesized_uid_reference.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [i for i in records if get_badread_strand_id(i) in strand_ids_synthesized]\n",
    "#records = filter_junk_reads(records)\n",
    "seqs = [str(i.seq) for i in records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ID reference functions\n",
    "\n",
    "def get_badread_strand_id(record):\n",
    "    return record.description.split()[1].split(',')[0]\n",
    "\n",
    "def get_strand_reference(strand_record, strand_index=True):\n",
    "\n",
    "    strand_id = get_badread_strand_id(strand_record)\n",
    "\n",
    "    if strand_id in strand_ids_synthesized:\n",
    "        if strand_index:\n",
    "            return original_strand_ids.index(strand_ids_synthesized[strand_id])\n",
    "        else: \n",
    "            return original_strands[original_strand_ids.index(strand_ids_synthesized[strand_id])]\n",
    "    print(\"Invalid Strand ID!\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edit_distance_matrix(strands):\n",
    "    \"\"\"\n",
    "    Returns the edit distance matrix for the strands\n",
    "    O(n^2)\n",
    "    \"\"\"\n",
    "\n",
    "    n_strands = len(strands)\n",
    "    edit_distance_matrix = np.zeros([n_strands, n_strands])\n",
    "    for i in range(n_strands - 1):\n",
    "        for j in range(i + 1, n_strands):\n",
    "            edit_distance_matrix[i,j] = edit_distance_matrix[j, i] = ratio(strands[i], strands[j])\n",
    "\n",
    "    return edit_distance_matrix\n",
    "\n",
    "def calculate_centroid(strands):\n",
    "    edit_distance_matrix = get_edit_distance_matrix(strands)\n",
    "\n",
    "    distances = [sum(edit_distance_matrix[i, :]) for i in range(len(edit_distance_matrix))]\n",
    "    return strands[distances.index(min(distances))]\n",
    "\n",
    "\n",
    "def get_mean_edit_distance_cluster(edit_distance_matrix):\n",
    "    distances = [sum(edit_distance_matrix[i, :]) for i in range(len(edit_distance_matrix))]\n",
    "    return np.mean(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_statistics(records):\n",
    "    \"\"\"\n",
    "    Given a sample of the data, I want to get the following\n",
    "    1. Number of strands that don't match to shit\n",
    "    2. Average number of unique matches per strand\n",
    "    3. Std number of unique matches per strand\n",
    "    3. Total number of good strands\n",
    "    \"\"\"\n",
    "\n",
    "    rec_threshold = 0.85\n",
    "    strands_by_index = np.zeros(len(original_strands))\n",
    "    unmatched = 0\n",
    "    junk = 0\n",
    "    seqs = []\n",
    "    \n",
    "    for record in records:\n",
    "\n",
    "        try:\n",
    "            strand_id = utils.get_badread_strand_id(record)\n",
    "            synthesized_id = strand_ids_synthesized[strand_id]\n",
    "            index = original_strand_ids.index(synthesized_id)\n",
    "            #index = original_strand_ids[strand_ids_synthesized[utils.get_badread_strand_id(record)]]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        seq = str(record.seq)\n",
    "        revseq = str(utils.reverse_complement(seq))\n",
    "\n",
    "        if ratio(seq, original_strands[index]) > rec_threshold:\n",
    "            strands_by_index[index] += 1\n",
    "            seqs.append(seq)\n",
    "        elif ratio(revseq, original_strands[index]) > rec_threshold:\n",
    "            strands_by_index[index] += 1\n",
    "            seqs.append(revseq)\n",
    "        else:\n",
    "            unmatched += 1\n",
    "\n",
    "    return unmatched, np.mean(strands_by_index), np.std(strands_by_index), seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched, strands_by_index_1, strands_by_index_2, seqs = get_sample_statistics(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total strands 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:00<00:00, 93677.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To catch the adapters\n",
    "clusters, centroids = heirarchal_clustering.cluster_trivial(seqs, similarity_threshold=0.85, use_centroids=False, analysis=False)\n",
    "clustered_seqs = [[seqs[i] for i in j] for j in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters 6\n",
      "Number of clusters in the right range 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of clusters {len(clusters)}\")\n",
    "print(f\"Number of clusters in the right range {len([i for i in clusters if len(i) > 12])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters = [i for i in clusters if len(i) > 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = get_sort_by_sublists_length(clusters)\n",
    "sorted_clusters = [clusters[i] for i in indices]\n",
    "sorted_centroids = [centroids[i] for i in indices]\n",
    "sorted_clustered_seqs = [clustered_seqs[i] for i in indices]\n",
    "\n",
    "centroids = sorted_centroids\n",
    "clusters = sorted_clusters\n",
    "clustered_seqs = sorted_clustered_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 59, 56, 50, 47, 1]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in clustered_seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtien edit distance works pretty well. That's my metric. Kmeans works for now. I'll bring down junk reads etc, remove adapters, and test it again to see how well aggregation works.\n",
    "Maybe try DBscan with Levenshtien distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9406175771971497 recovered by 0\n",
      "0.9280742459396751\n",
      "0.27\n",
      "\n",
      "4 0.9586374695863747 recovered by 1\n",
      "0.9501187648456058\n",
      "0.225\n",
      "\n",
      "3 0.9582309582309583 recovered by 2\n",
      "0.9345794392523364\n",
      "0.215\n",
      "\n",
      "2 0.9304556354916067 recovered by 3\n",
      "0.9195402298850575\n",
      "0.265\n",
      "\n",
      "0 0.9408983451536643 recovered by 4\n",
      "0.9324009324009324\n",
      "0.29\n",
      "\n",
      "2 0.9095238095238095 recovered by 5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_guessed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_rec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m recovered by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat())\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#print(len(i))\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m guess \u001b[38;5;241m=\u001b[39m \u001b[43mheirarchal_clustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclustered_seqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(ratio(guess, original_strands[best_guessed]))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_recovery_percentage(guess, original_strands[best_guessed]))\n",
      "File \u001b[1;32mc:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\heirarchal_clustering.py:118\u001b[0m, in \u001b[0;36mmake_prediction\u001b[1;34m(cluster, sample_size)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_prediction\u001b[39m(cluster, sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m--> 118\u001b[0m     cluster \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m majority_merge(multiple_alignment_muscle(cluster))\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\random.py:430\u001b[0m, in \u001b[0;36mRandom.sample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    428\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[0;32m    432\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "# And then merge and validate for top 5\n",
    "\n",
    "original_strand_guessed_best = []\n",
    "\n",
    "for ind, i in enumerate(centroids):\n",
    "    \n",
    "    best_guessed = 0\n",
    "    best_rec = 0.0\n",
    "    for k, original_strand in enumerate(original_strands):\n",
    "        rec = ratio(i, original_strand)\n",
    "\n",
    "        if rec > best_rec:\n",
    "            best_guessed = k\n",
    "            best_rec = rec\n",
    "        \n",
    "        rec = ratio(utils.reverse_complement(i), original_strand)\n",
    "\n",
    "        if rec > best_rec:\n",
    "            best_guessed = k\n",
    "            best_rec = rec\n",
    "        \n",
    "\n",
    "    original_strand_guessed_best.append(best_guessed)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    print(f\"{best_guessed} {best_rec} recovered by {ind}\".format())\n",
    "    #print(len(i))\n",
    "    guess = heirarchal_clustering.make_prediction(clustered_seqs[ind], sample_size=10)\n",
    "    print(ratio(guess, original_strands[best_guessed]))\n",
    "    print(get_recovery_percentage(guess, original_strands[best_guessed]))\n",
    "    print()\n",
    "\n",
    "\n",
    "    #print(f\"{len(clusters[ind])} elements in the cluster\")\n",
    "    #print(f\"{strand_filtering.sum_entropies(i)} long run\")\n",
    "    #print(f\"{np.mean([len(i) for i in clustered_seqs[ind]])} mean length of strand in cluster\")\n",
    "    #print(f\"{np.std([len(i) for i in clustered_seqs[ind]])} mean std of strand in cluster\")\n",
    "    #print(f\"{get_mean_edit_distance_cluster(distance_matrices[ind])} mean edit distance within cluster\")\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:00, 1182.65it/s]\n"
     ]
    }
   ],
   "source": [
    "clusters_2 = heirarchal_clustering.cluster_trivial(centroids, similarity_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_seqs.sort(key=len, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.5110294117647058 recovered by 0\n",
      "\n",
      "3 1.0 recovered by 1\n",
      "\n",
      "1 1.0 recovered by 2\n",
      "\n",
      "0 0.5243445692883895 recovered by 3\n",
      "\n",
      "4 1.0 recovered by 4\n",
      "\n",
      "2 1.0 recovered by 5\n",
      "\n",
      "0 0.5183823529411765 recovered by 6\n",
      "\n",
      "0 0.4856115107913669 recovered by 7\n",
      "\n",
      "0 1.0 recovered by 8\n",
      "\n",
      "4 0.5054545454545455 recovered by 9\n",
      "\n",
      "1 0.9900497512437811 recovered by 10\n",
      "\n",
      "4 0.46875 recovered by 11\n",
      "\n",
      "1 0.3701657458563536 recovered by 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And then merge and validate for top 5\n",
    "\n",
    "original_strand_guessed_best = []\n",
    "\n",
    "for i in range(len(clustered_seqs)):\n",
    "    if len(clustered_seqs) > 15:\n",
    "        guess = heirarchal_clustering.make_prediction(clustered_seqs[i], sample_size=15)\n",
    "    else:\n",
    "        guess = heirarchal_clustering.make_prediction(clustered_seqs[i], sample_size=len(clustered_seqs[i]))\n",
    "    \n",
    "    best_guessed = 0\n",
    "    best_rec = 0.0\n",
    "    for k, original_strand in enumerate(original_strands):\n",
    "        rec = get_recovery_percentage(guess, original_strand)\n",
    "        rec = align(guess, original_strand)\n",
    "\n",
    "        if rec > best_rec:\n",
    "            best_guessed = k\n",
    "            best_rec = rec\n",
    "\n",
    "    original_strand_guessed_best.append(best_guessed)\n",
    "    print(f\"{best_guessed} {best_rec} recovered by {i}\".format())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
