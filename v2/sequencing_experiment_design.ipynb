{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing Experiment Design\n",
    "Comparing capping and no capping for a coupling rate of 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from seq_stat import align\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aligned_clustering import conduct_align_clustering\n",
    "from utils import get_original_strands, read_synthesized_strands_from_file\n",
    "import random\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading original strands and synthesized strands\n",
    "\n",
    "original_strands_filepath = r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-10 09.39.03.129913\\original_strands.txt\"\n",
    "synthesized_strands_filepath = r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\runs\\2025-01-10 09.39.03.129913\\synthesized.fasta\"\n",
    "\n",
    "# Read original strands from the file\n",
    "original_strand_ids, coupling_rates, capping_flags, original_strands = get_original_strands(original_strand_filepath=original_strands_filepath)\n",
    "\n",
    "# Read synthesised strands from file - 360,000 of these\n",
    "synthesized_strands, synthesized_strand_ids = read_synthesized_strands_from_file(synthesized_strands_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating padded file for Badread\n",
    "\n",
    "def generate_random_bases(n_bases):\n",
    "\n",
    "    bases = ['A', 'C', 'T', 'G']\n",
    "    return \"\".join([random.choice(bases) for i in range(n_bases)])\n",
    "    \n",
    "\n",
    "def create_badread_data(synthesized_strands, synthesized_strand_ids, write_filename='padded_synth.fasta'):\n",
    "\n",
    "    random.shuffle(synthesized_strands)\n",
    "    synthesized_padded_dict = {}\n",
    "    with open(write_filename, 'w') as f:\n",
    "        for strand, base_id in zip(synthesized_strands, synthesized_strand_ids):\n",
    "            \n",
    "            strand = generate_random_bases(200) + strand\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            f.write(f\">{id}, {unique_id}\\n\")\n",
    "            f.write(strand + '\\n\\n')\n",
    "            synthesized_padded_dict[unique_id] = strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_badread_data(synthesized_strands=synthesized_strands, synthesized_strand_ids=synthesized_strand_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Post Badread data processing\n",
    "\n",
    "def parse_biopython(input_fastq):\n",
    "    for record in SeqIO.parse(input_fastq, 'fastq'):\n",
    "        yield record\n",
    "        \n",
    "def postprocess_sequencing_data(fastq_filepath, original_strand_ids, original_strands):\n",
    "    \"\"\"\n",
    "    The record description contains the strand starting, ending and orientation\n",
    "    \"\"\"\n",
    "    sequenced_strands = []\n",
    "    for i, record in tqdm(enumerate(parse_biopython(fastq_filepath))):\n",
    "        strand_id = record.description.split()[1].split(',')[0]\n",
    "        if strand_id in original_strand_ids:\n",
    "            target_strand = original_strands[original_strand_ids.index(strand_id)]\n",
    "        else:\n",
    "            continue\n",
    "        #aligned, identity = align(target_strand, record.seq)\n",
    "\n",
    "        \n",
    "        #if identity > 0.7:\n",
    "        #   sequenced_strands.append(str(aligned.replace('-', '')))\n",
    "\n",
    "        sequenced_strands.append(str(record.seq))\n",
    "\n",
    "    return sequenced_strands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048c3173a12f4ca1ac86ac11e9e31439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fastq_filepath = r\"C:\\Users\\Parv\\Doc\\RA\\Projects\\incomplete_cycles\\v2\\reads.fastq\\reads.fastq\"\n",
    "sequenced_strands = postprocess_sequencing_data(fastq_filepath=fastq_filepath, original_strand_ids=original_strand_ids, original_strands=original_strands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clustering \n",
    "\n",
    "def cluster_data(original_strands, sequenced_strands):\n",
    "    recoveries = conduct_align_clustering(\n",
    "            original_strand=original_strands,\n",
    "            trimmed_seqs=sequenced_strands,\n",
    "            display=False,\n",
    "            multiple=True\n",
    "        )\n",
    "    return list(recoveries['recoveries'].values())\n",
    "\n",
    "# Post Clustering Analysis\n",
    "\n",
    "def post_process_results(recoveries_strands, capping_flags, coupling_rates):\n",
    "\n",
    "    columns = [\n",
    "    'capping',\n",
    "    'coupling_rate',\n",
    "    'pool_recovery'\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(np.array([capping_flags, coupling_rates, recoveries_strands]).T, columns=columns)\n",
    "    df_capping = df.loc[df['capping'] == 'True']\n",
    "    df_no_capping = df.loc[df['capping'] ==  'False']\n",
    "\n",
    "    df_capping = df_capping.drop(['capping'], axis=1)\n",
    "    df_no_capping = df_no_capping.drop(['capping'], axis=1)\n",
    "\n",
    "    return df_capping, df_no_capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'post_process_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m sampled_sequenced_strands \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(sequenced_strands, \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      2\u001b[0m recoveries_strands \u001b[38;5;241m=\u001b[39m cluster_data(original_strands\u001b[38;5;241m=\u001b[39moriginal_strands, sequenced_strands\u001b[38;5;241m=\u001b[39msampled_sequenced_strands)\n\u001b[1;32m----> 3\u001b[0m df_capping, df_no_capping \u001b[38;5;241m=\u001b[39m \u001b[43mpost_process_results\u001b[49m(recoveries_strands\u001b[38;5;241m=\u001b[39mrecoveries_strands, capping_flags\u001b[38;5;241m=\u001b[39mcapping_flags, coupling_rates\u001b[38;5;241m=\u001b[39mcoupling_rates)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'post_process_results' is not defined"
     ]
    }
   ],
   "source": [
    "sampled_sequenced_strands = random.sample(sequenced_strands, 1000)\n",
    "recoveries_strands = cluster_data(original_strands=original_strands, sequenced_strands=sampled_sequenced_strands)\n",
    "df_capping, df_no_capping = post_process_results(recoveries_strands=recoveries_strands, capping_flags=capping_flags, coupling_rates=coupling_rates)\n",
    "print(df_capping)\n",
    "print(df_no_capping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
